<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Deep source separation for speech using generative models | Suki Zhang (张苏琦)</title> <meta name="author" content="Suki Zhang (张苏琦)"> <meta name="description" content="Bachelor's thesis"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%83%F0%9F%8F%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://saki-77.github.io/projects/1_project/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Suki </span>Zhang (张苏琦)</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Deep source separation for speech using generative models</h1> <p class="post-description">Bachelor's thesis</p> </header> <article> <p><strong>Acknowledgement</strong></p> <p>This is a reorganized and concise version of my bachelor’s thesis at XJTLU where I was supervised by Prof. Yin Cao. I sincerely thank Prof. Cao for his encouragement, support, and invaluable guidance throughout this journey.</p> <p><strong>Abstract</strong></p> <p>Extracting the voice of desired speaker from a noisy speech is a challenging task. Speech enhancement (SE) aims to achieve this by enhancing speech that is degraded by noise. Recent studies have explored applying deep learning-based SE models for real-time usage. For instance, DeepFilterNet has been proposed for resource-constrained embedded devices, offering real-time capabilities due to its lightweight architecture. However, objective function in many of these deep neural networks is not designed to directly optimize evaluation metrics of a target task, and thus, may not always guide the networks to achieve satisfied evaluation score even if the objective loss is optimized.</p> <p>To overcome this issue, inspired by MetricGAN, we proposed to combine a metric-estimated network with DeepFilterNet2 together as a metric generative adversarial network, where the DeepFilterNet2 is seen as generator and optimized by evaluation metric. By utilizing this network structure, the estimated speech not only becomes more intelligible but is further enhanced to sound more natural. Moreover, we introduce additional distortion methods to further increase the diversity of the training data and make it more closely match noisy speech in real environments. We evaluate our proposed methods’ effectiveness by quantitatively analyzing its performance on Voice Bank + DEMAND dataset and LibriMix blind test set.</p> <p><strong>Introduction</strong></p> <p>Speech enhancement is a critical task involving the separation of target speech from background interference. In real-world applications, the quality and intelligibility of speech perceived by users heavily rely on the effectiveness of SE systems. These systems are critical components in contemporary automatic speech recognition (ASR), telecommunications, and hearing aid technologies. The importance of SE frameworks is highlighted by the significant volume of ongoing research aimed at improving the performance of existing SE systems. Most of these advancements utilize the latest developments in deep learning (DL) methods and the growing availability of speech data.</p> <p>Common deep learning-based approaches typically determine the relationship between noisy and clean signals using discriminative methods either in the time-frequency (T-F) domain or the time domain. In T-F domain methods, the initial step involves transforming time-domain speech signals into spectral features typically using a short-time Fourier transform (STFT). The mapping from noisy to clean spectral features is established through a direct mapping function [5], or a masking function [6, 7]. The improved spectral features 4 are then reconstructed into time-domain waveforms by incorporating the phase of the noisy speech through an inverse STFT operation. In comparison to T-F domain methods, time domain methods that learn to directly map noisy to clean in waveforms, have demonstrated an ability to circumvent distortions arising from inaccurate phase information [8, 9].</p> <p>In recent years, various generative approaches have been explored and proved be effectiveness for the SE task. For speech enhancement, likelihood-based models such as generative adversarial networks (GAN) [10, 11, 12, 13, 14, 15, 16], Bayesian WaveNet [17], autoregressive models [18], variational autoencoders (VAE) [19, 20], and flow-based models [21, 22] have been widely employed. Compared to discriminative approaches, generative models are more robust to unseen scenarios and tend to produce more natural speech.</p> <p>In the context of utilizing GANs for SE, the process of enhancing speech involves a generator which is responsible for learning clean speech, while the discriminator plays a role in distinguishing between authentic and generated signals. By discerning real from fake signals, the discriminator conveys information to the generator. This enables the generator to learn and generate output that closely mirrors the realistic distribution of clean signals. Consequently, it helps to reconstruct the target speech from the mixed speech that has significant interference in it, where only partial information of the noisy speech is related to the clean speech.</p> <p>Commonly, the objective functions used in SE are to simply estimate the $L_{p}$ norm distance between the enhanced spectrogram and the target spectrogram, as are most of the loss functions used in adversarial training. However, a smaller distance does not always mean higher speech quality. To address this issue, the MetricGAN [16] method has been proposed. In this model, the discriminator is a simple network that simulates the PESQ score. Thus, the generator is optimized directly from the evaluation metric scores that are 5 learned by the discriminator. Through this approach, the quality as well as intelligibility of speech generated by GAN-based models could be improved.</p> <p>Nevertheless, many GAN-based algorithms cannot truly operate in real-time on embedded devices due to their high computational complexity, or the requirement of large time buffers for real-time use due to temporal convolutions or attention mechanisms. Both scenarios render these methods impractical for embedded devices. In response to this issue, solutions like GaGNet [23] or DeepFilterNet2 [24] have introduced smaller and more efficient architectures. This characteristic is crucial for speech enhancement (SE) on source-limited embedded devices, as system complexity must be controlled.</p> <p>In this work, we proposed a GAN-based approach to improve the performance of real-time SE algorithms. Specifically, we employ DeepFilterNet2 as generator, a real-time algorithm with a lightweight framework. The discriminator of our model, on the other hand, is a simple network for predicting PESQ values using the one proposed in MetricGAN [16]. In concrete terms, we combine these two networks and perform adversarial training to exploit the following properties:</p> <p>Deepfilternet could be run in real time on resource-constrained embedded devices. Using a metric discriminator can directly connect the network’s optimization objectives with audio metrics. The generative power of GAN models.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Suki Zhang (张苏琦). Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>